{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV 'tweets.csv' creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "def process_tweets(xml_file, csv_file):\n",
    "    \"\"\"\n",
    "    Procesa un archivo XML de tweets y crea un archivo CSV. Concatena todo el texto del tweet,\n",
    "    incluso si hay múltiples etiquetas <sentiment>.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    header = ['tweet_text', 'aspects']\n",
    "\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        \n",
    "\n",
    "        for tweet in root.findall('tweet'):\n",
    "            tweet_text = []\n",
    "            for element in tweet.iter():\n",
    "                if element.text:\n",
    "                    tweet_text.append(element.text.strip()) \n",
    "                if element.tail and element.tail != '\\n':\n",
    "                    tweet_text.append(element.tail.strip()) \n",
    "            aspects = []\n",
    "\n",
    "            for sentiment in tweet.findall('.//sentiment'):\n",
    "                aspects.append(sentiment.get('aspect'))\n",
    "\n",
    "            row = [tweet_text, ','.join(aspects)]\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "xml_file = '../Data/socialtv-train-tagged.xml' # Reemplaza con la ruta a tu archivo XML\n",
    "csv_file = 'tweets.csv' # Reemplaza con la ruta deseada para el archivo CSV\n",
    "process_tweets(xml_file, csv_file)\n",
    "\n",
    "print(f\"Archivo CSV '{csv_file}' creado con éxito.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = '../Data/stompol-test-tagged.xml' # Reemplaza con la ruta a tu archivo XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Carga del dataset\n",
    "df = pd.read_csv(\"tu_dataset.csv\") # Reemplaza \"tu_dataset.csv\" con el nombre de tu archivo\n",
    "\n",
    "# 2. Preprocesamiento del dataset\n",
    "# Convertir la columna 'aspecto' a una lista de etiquetas (necesario para BERT)\n",
    "def etiqueta_aspectos(texto_aspectos):\n",
    "    \"\"\"Convierte la lista de aspectos en una lista de etiquetas para tokenización BERT.\"\"\"\n",
    "    aspectos = eval(texto_aspectos) # eval() es inseguro si tu CSV no es de confianza. Considera una función de parsing más robusta\n",
    "    etiquetas = []\n",
    "    for i, palabra in enumerate(texto.split()):\n",
    "        if palabra in aspectos:\n",
    "          etiquetas.append(\"B-ASPECTO\") # B-ASPECTO indica el inicio de un aspecto\n",
    "        else:\n",
    "          etiquetas.append(\"O\") # O indica que no es un aspecto\n",
    "\n",
    "    return etiquetas\n",
    "\n",
    "\n",
    "# Aplicar la función a cada fila\n",
    "df['etiquetas'] = df.apply(lambda row: etiqueta_aspectos(row['aspecto']), axis=1)\n",
    "\n",
    "# Limpieza adicional (opcional): elimina filas con etiquetas vacías o problemas de formato\n",
    "df.dropna(subset=['etiquetas'], inplace=True)\n",
    "df = df[df['etiquetas'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "\n",
    "# 3. Crear un Dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 4. Dividir el dataset en entrenamiento y validación\n",
    "train_dataset, eval_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 5. Cargar el modelo BERT y el tokenizer\n",
    "model_name = \"bert-base-uncased\" # Puedes usar otro modelo BERT pre-entrenado\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name, num_labels=2) # 2 etiquetas: O y B-ASPECTO\n",
    "\n",
    "\n",
    "# 6. Tokenización del dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, is_split_into_words=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "# 7. Alinear las etiquetas con los tokens\n",
    "def align_labels_with_tokens(examples):\n",
    "  labels = []\n",
    "  for i, label in enumerate(examples[\"etiquetas\"]):\n",
    "      word_ids = examples[\"word_ids\"][i]\n",
    "      previous_word_idx = None\n",
    "      label_ids = []\n",
    "      for word_idx in word_ids:\n",
    "          if word_idx is None:\n",
    "              label_ids.append(-100)\n",
    "          elif word_idx != previous_word_idx:\n",
    "              label_ids.append(label[word_idx])\n",
    "          else:\n",
    "              label_ids.append(-100)\n",
    "          previous_word_idx = word_idx\n",
    "      labels.append(label_ids)\n",
    "  examples[\"labels\"] = labels\n",
    "  return examples\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(align_labels_with_tokens, batched=True)\n",
    "tokenized_eval_dataset = tokenized_eval_dataset.map(align_labels_with_tokens, batched=True)\n",
    "\n",
    "\n",
    "# 8. Definir los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3, # Ajusta el número de épocas según tus necesidades\n",
    "    per_device_train_batch_size=8, # Ajusta el tamaño del batch\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # Agrega otros argumentos de entrenamiento según sea necesario.\n",
    ")\n",
    "\n",
    "\n",
    "# 9. Entrenar el modelo\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 10. Guardar el modelo entrenado\n",
    "trainer.save_model(\"./modelo_entrenado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
