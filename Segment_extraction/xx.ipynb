{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer() # Inicializa un stemmer de Porter para reducir palabras a su raíz\n",
    "\n",
    "# Cargar el modelo en español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\") # Carga un modelo de procesamiento de lenguaje natural en español\n",
    "\n",
    "\n",
    "# Dada la oración y el aspecto, extrae el segmento donde se encuentra el aspecto\n",
    "def extraer_segmento(oracion,aspecto):\n",
    "    \"\"\"\n",
    "    Extrae el segmento de una oración que contiene un aspecto dado. Combina varias funciones para identificar y retornar la porción relevante de texto.\n",
    "    \"\"\"\n",
    "    dependencias = extraer_dependencias(oracion, aspecto) # Obtiene las dependencias sintácticas del aspecto en la oración\n",
    "    lista = palabras_sin_repetir(dependencias) # Elimina las palabras repetidas de la lista de dependencias (función no definida aquí)\n",
    "    segmento = combinacion(oracion,lista) # Combina las palabras en un segmento coherente (función no definida aquí)\n",
    "\n",
    "    return convertir_lista_oracion(segmento) # Convierte la lista de palabras resultante en una oración (función no definida aquí)\n",
    "\n",
    "\n",
    "def extraer_dependencias(texto_opinion, palabra_clave):\n",
    "    \"\"\"\n",
    "    Extrae información sintáctica relacionada con una palabra clave en un texto. Identifica y devuelve frases relacionadas con el aspecto.\n",
    "    \"\"\"\n",
    "    doc_analizado = nlp(texto_opinion) # Analiza el texto usando el modelo de spaCy\n",
    "    frases_extraidas = [] # Inicializa una lista para guardar las frases extraídas\n",
    "\n",
    "    for palabra_actual in doc_analizado: # Itera sobre cada palabra del texto analizado\n",
    "        if palabra_actual.text == palabra_clave or palabra_actual.text == stemmer.stem(palabra_clave) : # Busca la palabra clave o su raíz\n",
    "\n",
    "            palabras_asociadas = [] # Lista para guardar las palabras asociadas a la palabra clave\n",
    "\n",
    "            # Extrae palabras a la izquierda y derecha de la palabra clave\n",
    "            palabras_asociadas.extend(extraer_palabras_adyacentes(palabra_actual, 'left'))\n",
    "            palabras_asociadas.append(palabra_actual.text)\n",
    "            palabras_asociadas.extend(extraer_palabras_adyacentes(palabra_actual, 'right'))\n",
    "\n",
    "            palabra_principal = palabra_actual.head # Encuentra la palabra principal (la raíz sintáctica) de la palabra clave\n",
    "\n",
    "            # Extrae sujetos y objetos directos relacionados con la palabra principal\n",
    "            palabras_asociadas.extend(extraer_palabras_adyacentes(palabra_principal, 'left'))\n",
    "            palabras_asociadas.append(palabra_principal.text)\n",
    "            palabras_asociadas.extend(extraer_palabras_adyacentes(palabra_principal, 'right'))\n",
    "\n",
    "            # Agrega n-gramas, contexto y la frase extraída a la lista de frases\n",
    "            frases_extraidas.append(palabras_asociadas)\n",
    "            frases_extraidas.append(n_gramas(texto_opinion, palabra_clave, 3))\n",
    "            frases_extraidas.append(extraccion_contexto(texto_opinion, palabra_clave))\n",
    "\n",
    "    return frases_extraidas # Devuelve la lista de frases extraídas\n",
    "\n",
    "\n",
    "# Funciones auxiliares para extraer dependencias\n",
    "\n",
    "def extraer_palabras_adyacentes(token_actual, direccion):\n",
    "    \"\"\"\n",
    "    Extrae las palabras a la izquierda o derecha de un token, incluyendo adjetivos relacionados con sustantivos.\n",
    "\n",
    "    Args:\n",
    "        token_actual: El token de referencia.\n",
    "        direccion: 'left' para palabras a la izquierda, 'right' para palabras a la derecha.\n",
    "\n",
    "    Returns:\n",
    "        Una lista de palabras.\n",
    "    \"\"\"\n",
    "    palabras = []\n",
    "    adyacentes = token_actual.lefts if direccion == 'left' else token_actual.rights\n",
    "    for token_adyacente in adyacentes:\n",
    "        palabras.append(token_adyacente.text)\n",
    "        if token_adyacente.dep_ == 'NOUN':\n",
    "            for token_adjetivo in token_adyacente.lefts:\n",
    "                if token_adjetivo.dep_ == 'ADJ':\n",
    "                    palabras.append(token_adjetivo.text)\n",
    "            for token_adjetivo in token_adyacente.rights:\n",
    "                if token_adjetivo.dep_ == 'ADJ':\n",
    "                    palabras.append(token_adjetivo.text)\n",
    "    return palabras\n",
    "\n",
    "def n_gramas(oracion, palabra, numero):\n",
    "    \"\"\"\n",
    "    Extrae n-gramas (secuencias de n palabras) alrededor de una palabra clave en una oración.\n",
    "    \"\"\"\n",
    "    palabras = limpiar_oracion(oracion).split() # Limpia la oración y la divide en palabras \n",
    "    index_palabra = [] # Inicializa una lista para guardar el índice de la palabra clave en la lista de palabras\n",
    "\n",
    "    for index, word in enumerate(palabras): # Itera sobre las palabras para encontrar la palabra clave\n",
    "        if word.lower() == palabra.lower() or word.lower() == stemmer.stem(palabra.lower()): # Busca la palabra clave o su raíz\n",
    "            index_palabra = index\n",
    "            break\n",
    "\n",
    "    palabras_anteriores = palabras[max(0, index_palabra - numero):index_palabra] # Extrae las palabras anteriores a la palabra clave\n",
    "    palabras_posteriores = palabras[index_palabra + 1:index_palabra + 1 + numero] # Extrae las palabras posteriores a la palabra clave\n",
    "\n",
    "    return palabras_anteriores + [palabra] + palabras_posteriores # Devuelve una lista con las palabras anteriores, la palabra clave y las palabras posteriores\n",
    "\n",
    "\n",
    "def extraccion_contexto(oracion, palabra_relevante):\n",
    "    \"\"\"\n",
    "    Extrae el contexto de una palabra clave en una oración, incluyendo modificadores.\n",
    "    \"\"\"\n",
    "    doc = nlp(oracion) # Analiza la oración con spaCy\n",
    "    token_palabra_relevante = None # Inicializa la variable para guardar el token de la palabra clave\n",
    "    for token in doc: # Busca el token de la palabra clave en la oración analizada\n",
    "        if token.text.lower() == palabra_relevante:\n",
    "            token_palabra_relevante = token\n",
    "            break\n",
    "\n",
    "    if not token_palabra_relevante: # Si no se encuentra la palabra clave\n",
    "        return \"Palabra relevante no encontrada en la oración\"\n",
    "\n",
    "    modificadores = [] # Inicializa una lista para guardar los modificadores de la palabra clave\n",
    "    for token in doc: # Itera sobre los tokens de la oración analizada\n",
    "        # Busca modificadores de la palabra clave (adjetivos, adverbios, etc.)\n",
    "        if token.head == token_palabra_relevante and token.dep_ in [\"amod\", \"advmod\", \"nsubj\", \"nummod\", \"det\"]: \n",
    "            modificadores.append(token.text)\n",
    "\n",
    "    return modificadores # Devuelve la lista de modificadores\n",
    "\n",
    "\n",
    "def limpiar_oracion(oracion):\n",
    "\n",
    "    # Usar una expresión regular para encontrar y reemplazar todos los caracteres que no sean letras\n",
    "    oracion_limpia = re.sub(r'[^a-zA-Z\\sáéíóúüÁÉÍÓÚÜñÑ]', ' ', oracion) \n",
    "    # Devolver la oración limpia\n",
    "    return oracion_limpia\n",
    "\n",
    "def palabras_sin_repetir(lista_de_listas):\n",
    "    palabras = set()  # Utilizamos un conjunto para evitar palabras duplicadas  \n",
    "    for sublista in lista_de_listas:\n",
    "        for palabra in sublista:\n",
    "            palabras.add(palabra) \n",
    "\n",
    "    return list(palabras)  # Convertimos el conjunto de palabras de nuevo a una lista y la devolvemos\n",
    "\n",
    "def combinacion(oracion, lista_palabras):\n",
    "    \"\"\"\n",
    "    Extrae las palabras de una oración que están presentes en una lista dada, \n",
    "    evitando duplicados y manejando algunas variaciones simples.\n",
    "\n",
    "    Args:\n",
    "        oracion: La oración de entrada como una cadena de texto.\n",
    "        lista_palabras: Una lista de palabras a buscar en la oración.\n",
    "\n",
    "    Returns:\n",
    "        Una lista de palabras de la oración que se encontraron en la lista de palabras, sin duplicados.\n",
    "    \"\"\"\n",
    "    resultado = [] # Inicializa una lista vacía para almacenar las palabras encontradas.\n",
    "\n",
    "    for token in oracion.split(): # Itera sobre cada palabra (token) en la oración.\n",
    "        if (token in lista_palabras) or (token[0:-1] in lista_palabras): # Verifica si el token o su forma sin el último carácter está en la lista de palabras.\n",
    "            if not token in resultado: # Verifica si el token ya está en la lista de resultados para evitar duplicados.\n",
    "                resultado.append(token) # Agrega el token a la lista de resultados.\n",
    "\n",
    "    return resultado # Devuelve la lista de palabras encontradas.\n",
    "\n",
    "def convertir_lista_oracion(lista_palabras):\n",
    "\n",
    "    oracion = \" \".join(lista_palabras)  # Unir las palabras con espacios\n",
    "    return oracion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "def process_dataset(input_filename, output_filename):\n",
    "    \"\"\"\n",
    "    Procesa un dataset y crea un nuevo CSV con las columnas 'texto', 'aspecto' y 'polaridad'.\n",
    "    \"\"\"\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile, open(output_filename, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile, delimiter=';')\n",
    "        writer = csv.writer(outfile, delimiter=',')\n",
    "        writer.writerow(['texto', 'aspecto', 'polaridad']) #Escribe la cabecera\n",
    "\n",
    "        next(reader) # Salta la cabecera del archivo de entrada\n",
    "\n",
    "        for row in reader:\n",
    "            text_tokens = eval(row[0]) #Evalua la cadena para obtener la lista de tokens\n",
    "            tags = [x for x in row[1].split(',')] # Convierte las tags a enteros\n",
    "            polarities = [x for x in row[2].split(',')] # Convierte las polaridades a enteros\n",
    "\n",
    "            sentence = ' '.join(text_tokens) # Junta los tokens para formar la oración\n",
    "\n",
    "\n",
    "            for i, tag in enumerate(tags):\n",
    "                if tag[1] == '1': #Si es un aspecto positivo o negativo (0 o 1)\n",
    "                    aspect = text_tokens[i]\n",
    "                    polarity = polarities[i][1]\n",
    " \n",
    "\n",
    "                    writer.writerow([sentence, aspect, polarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV procesado y guardado en: output.csv\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "input_file = '../Data/dataset_test_without_duplicates.csv'\n",
    "\n",
    "\n",
    "output_file = 'output.csv'\n",
    "process_dataset(input_file, output_file)\n",
    "\n",
    "print(f\"Archivo CSV procesado y guardado en: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV creado: output3.csv\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "def process_xml_to_csv(xml_filename, csv_filename):\n",
    "    \"\"\"\n",
    "    Procesa un archivo XML de reseñas y crea un archivo CSV.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    data = []\n",
    "    for review in root.findall('Review'):\n",
    "        for sentence in review.findall('sentences/sentence'):\n",
    "            text = sentence.find('text').text\n",
    "            for opinion in sentence.findall('Opinions/Opinion'):\n",
    "                target = opinion.get('target')\n",
    "                polarity = opinion.get('polarity')\n",
    "                if target is None:\n",
    "                    target = \"GENERAL\" # para opiniones sin un target especifico\n",
    "                data.append([text, target, 1 if polarity == 'positive' else 0])\n",
    "\n",
    "\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['texto', 'aspecto', 'polaridad'])\n",
    "        writer.writerows(data)\n",
    "\n",
    "#Ejemplo de uso: Reemplaza 'input.xml' y 'output.csv' con los nombres de tus archivos.\n",
    "xml_file = '../Data/SemEval/SemEval-2016ABSA Restaurants-Spanish_Test.xml'\n",
    "csv_file = 'output3.csv'\n",
    "process_xml_to_csv(xml_file, csv_file)\n",
    "print(f\"Archivo CSV creado: {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV creado: output3.csv\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "def process_xml_to_csv(xml_filename, csv_filename):\n",
    "    \"\"\"\n",
    "    Procesa un archivo XML de reseñas y crea un archivo CSV, extrayendo solo la primera palabra del aspecto.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    data = []\n",
    "    for review in root.findall('Review'):\n",
    "        for sentence in review.findall('sentences/sentence'):\n",
    "            text = sentence.find('text').text\n",
    "            for opinion in sentence.findall('Opinions/Opinion'):\n",
    "                target = opinion.get('target')\n",
    "                polarity = opinion.get('polarity')\n",
    "\n",
    "                if target is not None:\n",
    "                    aspect = target.split()[0] # Extrae la primera palabra\n",
    "                else:\n",
    "                    aspect = \"GENERAL\" # Para opiniones sin target específico\n",
    "\n",
    "                data.append([text, aspect, 1 if polarity == 'positive' else 0])\n",
    "\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['texto', 'aspecto', 'polaridad'])\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Ejemplo de uso: Reemplaza 'input.xml' y 'output.csv' con los nombres de tus archivos.\n",
    "xml_file = '../Data/SemEval/SemEval-2016ABSA Restaurants-Spanish_Test.xml'\n",
    "csv_file = 'output3.csv'\n",
    "process_xml_to_csv(xml_file, csv_file)\n",
    "print(f\"Archivo CSV creado: {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sep\n",
    "import spacy\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "stemmer = PorterStemmer() # Inicializa un stemmer de Porter para reducir palabras a su raíz\n",
    "\n",
    "# Cargar el modelo en español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\") # Carga un modelo de procesamiento de lenguaje natural en español\n",
    "\n",
    "\n",
    "# Dada la oración y el aspecto, extrae el segmento donde se encuentra el aspecto\n",
    "def extraer_segmento(oracion,aspecto):\n",
    "    \"\"\"\n",
    "    Extrae el segmento de una oración que contiene un aspecto dado. Combina varias funciones para identificar y retornar la porción relevante de texto.\n",
    "    \"\"\"\n",
    "    dependencias = extraer_dependencias(oracion, aspecto) # Obtiene las dependencias sintácticas del aspecto en la oración\n",
    "    lista = palabras_sin_repetir(dependencias) # Elimina las palabras repetidas de la lista de dependencias (función no definida aquí)\n",
    "    segmento = combinacion(oracion,lista) # Combina las palabras en un segmento coherente (función no definida aquí)\n",
    "\n",
    "    return convertir_lista_oracion(segmento) # Convierte la lista de palabras resultante en una oración (función no definida aquí)\n",
    "\n",
    "\n",
    "def extraer_dependencias(texto_opinion, palabra_clave):\n",
    "    \"\"\"\n",
    "    Extrae información sintáctica relacionada con una palabra clave en un texto. Identifica y devuelve frases relacionadas con el aspecto.\n",
    "    \"\"\"\n",
    "    doc_analizado = nlp(texto_opinion) # Analiza el texto usando el modelo de spaCy\n",
    "    frases_extraidas = [] # Inicializa una lista para guardar las frases extraídas\n",
    "\n",
    "    for palabra_actual in doc_analizado: # Itera sobre cada palabra del texto analizado\n",
    "        if palabra_actual.text == palabra_clave or palabra_actual.text == stemmer.stem(palabra_clave) : # Busca la palabra clave o su raíz\n",
    "\n",
    "            palabras_asociadas = [] # Lista para guardar las palabras asociadas a la palabra clave\n",
    "\n",
    "            # Extrae palabras a la izquierda y derecha de la palabra clave\n",
    "            palabras_asociadas.extend(extraer_palabras_adyacentes(palabra_actual, 'left'))\n",
    "            palabras_asociadas.append(palabra_actual.text)\n",
    "            palabras_asociadas.extend(extraer_palabras_adyacentes(palabra_actual, 'right'))\n",
    "\n",
    "            palabra_principal = palabra_actual.head # Encuentra la palabra principal (la raíz sintáctica) de la palabra clave\n",
    "\n",
    "            # Extrae sujetos y objetos directos relacionados con la palabra principal\n",
    "            palabras_asociadas.extend(extraer_palabras_adyacentes(palabra_principal, 'left'))\n",
    "            palabras_asociadas.append(palabra_principal.text)\n",
    "            palabras_asociadas.extend(extraer_palabras_adyacentes(palabra_principal, 'right'))\n",
    "\n",
    "            # Agrega n-gramas, contexto y la frase extraída a la lista de frases\n",
    "            frases_extraidas.append(palabras_asociadas)\n",
    "            frases_extraidas.append(n_gramas(texto_opinion, palabra_clave, 3))\n",
    "            frases_extraidas.append(extraccion_contexto(texto_opinion, palabra_clave))\n",
    "\n",
    "    return frases_extraidas # Devuelve la lista de frases extraídas\n",
    "\n",
    "\n",
    "# Funciones auxiliares para extraer dependencias\n",
    "\n",
    "def extraer_palabras_adyacentes(token_actual, direccion):\n",
    "    \"\"\"\n",
    "    Extrae las palabras a la izquierda o derecha de un token, incluyendo adjetivos relacionados con sustantivos.\n",
    "\n",
    "    Args:\n",
    "        token_actual: El token de referencia.\n",
    "        direccion: 'left' para palabras a la izquierda, 'right' para palabras a la derecha.\n",
    "\n",
    "    Returns:\n",
    "        Una lista de palabras.\n",
    "    \"\"\"\n",
    "    palabras = []\n",
    "    adyacentes = token_actual.lefts if direccion == 'left' else token_actual.rights\n",
    "    for token_adyacente in adyacentes:\n",
    "        palabras.append(token_adyacente.text)\n",
    "        if token_adyacente.dep_ == 'NOUN':\n",
    "            for token_adjetivo in token_adyacente.lefts:\n",
    "                if token_adjetivo.dep_ == 'ADJ':\n",
    "                    palabras.append(token_adjetivo.text)\n",
    "            for token_adjetivo in token_adyacente.rights:\n",
    "                if token_adjetivo.dep_ == 'ADJ':\n",
    "                    palabras.append(token_adjetivo.text)\n",
    "    return palabras\n",
    "\n",
    "def n_gramas(oracion, palabra, numero):\n",
    "    \"\"\"\n",
    "    Extrae n-gramas (secuencias de n palabras) alrededor de una palabra clave en una oración.\n",
    "    \"\"\"\n",
    "    palabras = limpiar_oracion(oracion).split() # Limpia la oración y la divide en palabras \n",
    "    index_palabra = [] # Inicializa una lista para guardar el índice de la palabra clave en la lista de palabras\n",
    "\n",
    "    for index, word in enumerate(palabras): # Itera sobre las palabras para encontrar la palabra clave\n",
    "        if word.lower() == palabra.lower() or word.lower() == stemmer.stem(palabra.lower()): # Busca la palabra clave o su raíz\n",
    "            index_palabra = index\n",
    "            break\n",
    "\n",
    "    palabras_anteriores = palabras[max(0, index_palabra - numero):index_palabra] # Extrae las palabras anteriores a la palabra clave\n",
    "    palabras_posteriores = palabras[index_palabra + 1:index_palabra + 1 + numero] # Extrae las palabras posteriores a la palabra clave\n",
    "\n",
    "    return palabras_anteriores + [palabra] + palabras_posteriores # Devuelve una lista con las palabras anteriores, la palabra clave y las palabras posteriores\n",
    "\n",
    "\n",
    "def extraccion_contexto(oracion, palabra_relevante):\n",
    "    \"\"\"\n",
    "    Extrae el contexto de una palabra clave en una oración, incluyendo modificadores.\n",
    "    \"\"\"\n",
    "    doc = nlp(oracion) # Analiza la oración con spaCy\n",
    "    token_palabra_relevante = None # Inicializa la variable para guardar el token de la palabra clave\n",
    "    for token in doc: # Busca el token de la palabra clave en la oración analizada\n",
    "        if token.text.lower() == palabra_relevante:\n",
    "            token_palabra_relevante = token\n",
    "            break\n",
    "\n",
    "    if not token_palabra_relevante: # Si no se encuentra la palabra clave\n",
    "        return \"Palabra relevante no encontrada en la oración\"\n",
    "\n",
    "    modificadores = [] # Inicializa una lista para guardar los modificadores de la palabra clave\n",
    "    for token in doc: # Itera sobre los tokens de la oración analizada\n",
    "        # Busca modificadores de la palabra clave (adjetivos, adverbios, etc.)\n",
    "        if token.head == token_palabra_relevante and token.dep_ in [\"amod\", \"advmod\", \"nsubj\", \"nummod\", \"det\"]: \n",
    "            modificadores.append(token.text)\n",
    "\n",
    "    return modificadores # Devuelve la lista de modificadores\n",
    "\n",
    "\n",
    "def limpiar_oracion(oracion):\n",
    "\n",
    "    # Usar una expresión regular para encontrar y reemplazar todos los caracteres que no sean letras\n",
    "    oracion_limpia = re.sub(r'[^a-zA-Z\\sáéíóúüÁÉÍÓÚÜñÑ]', ' ', oracion) \n",
    "    # Devolver la oración limpia\n",
    "    return oracion_limpia\n",
    "\n",
    "def palabras_sin_repetir(lista_de_listas):\n",
    "    palabras = set()  # Utilizamos un conjunto para evitar palabras duplicadas  \n",
    "    for sublista in lista_de_listas:\n",
    "        for palabra in sublista:\n",
    "            palabras.add(palabra) \n",
    "\n",
    "    return list(palabras)  # Convertimos el conjunto de palabras de nuevo a una lista y la devolvemos\n",
    "\n",
    "def combinacion(oracion, lista_palabras):\n",
    "    \"\"\"\n",
    "    Extrae las palabras de una oración que están presentes en una lista dada, \n",
    "    evitando duplicados y manejando algunas variaciones simples.\n",
    "\n",
    "    Args:\n",
    "        oracion: La oración de entrada como una cadena de texto.\n",
    "        lista_palabras: Una lista de palabras a buscar en la oración.\n",
    "\n",
    "    Returns:\n",
    "        Una lista de palabras de la oración que se encontraron en la lista de palabras, sin duplicados.\n",
    "    \"\"\"\n",
    "    resultado = [] # Inicializa una lista vacía para almacenar las palabras encontradas.\n",
    "\n",
    "    for token in oracion.split(): # Itera sobre cada palabra (token) en la oración.\n",
    "        if (token in lista_palabras) or (token[0:-1] in lista_palabras): # Verifica si el token o su forma sin el último carácter está en la lista de palabras.\n",
    "            if not token in resultado: # Verifica si el token ya está en la lista de resultados para evitar duplicados.\n",
    "                resultado.append(token) # Agrega el token a la lista de resultados.\n",
    "\n",
    "    return resultado # Devuelve la lista de palabras encontradas.\n",
    "\n",
    "def convertir_lista_oracion(lista_palabras):\n",
    "\n",
    "    oracion = \" \".join(lista_palabras)  # Unir las palabras con espacios\n",
    "    return oracion\n",
    "\n",
    "\n",
    "\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "\n",
    "def process_xml_to_csv(xml_filename, csv_filename):\n",
    "    \"\"\"\n",
    "    Procesa un archivo XML de reseñas y crea un archivo CSV, extrayendo solo la primera palabra del aspecto.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    data = []\n",
    "    for review in root.findall('Review'):\n",
    "        for sentence in review.findall('sentences/sentence'):\n",
    "            text = sentence.find('text').text\n",
    "            for opinion in sentence.findall('Opinions/Opinion'):\n",
    "                target = opinion.get('target')\n",
    "                polarity = opinion.get('polarity')\n",
    "\n",
    "                if target is not None:\n",
    "                    aspect = target.split()[0] # Extrae la primera palabra\n",
    "                else:\n",
    "                    aspect = \"GENERAL\" # Para opiniones sin target específico\n",
    "\n",
    "                if polarity == 'positive':\n",
    "                    pol = 1\n",
    "                elif polarity == \"neutral\":\n",
    "                    pol = 2\n",
    "                else:\n",
    "                    pol = 0\n",
    "\n",
    "                data.append([text, aspect, pol])\n",
    "\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow(['texto', 'aspecto', 'polaridad'])\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV creado: output.csv\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso: Reemplaza 'input.xml' y 'output.csv' con los nombres de tus archivos.\n",
    "xml_file = '../Data/SemEval/SemEval-2016ABSA Restaurants-Spanish_Test.xml'\n",
    "csv_file = 'output.csv'\n",
    "process_xml_to_csv(xml_file, csv_file)\n",
    "print(f\"Archivo CSV creado: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Añadir columna segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def procesar_csv(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Recorre un archivo CSV y imprime el texto y el aspecto de cada fila.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo: La ruta al archivo CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(ruta_archivo, sep=\";\")\n",
    "    df[\"Segmento\"] = \"\"\n",
    "    lista_segmentos = []\n",
    "    try:\n",
    "        for index, fila in df.iterrows():\n",
    "            texto = fila['review']\n",
    "            aspecto = fila['aspect']\n",
    "            if aspecto == \"vacio\":\n",
    "                segmento = texto\n",
    "            else:\n",
    "                segmento = extraer_segmento(texto,aspecto)\n",
    "                \n",
    "            lista_segmentos.append(segmento)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Archivo '{ruta_archivo}' no encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo: {e}\")\n",
    "\n",
    "    df[\"Segmento\"] = lista_segmentos\n",
    "    df.to_csv(ruta_archivo, index=False, sep=\";\")\n",
    "\n",
    "\n",
    "# Reemplaza 'tu_archivo.csv' con la ruta real a tu archivo CSV\n",
    "ruta_archivo = '../Segment_extraction/nuevo_archivo.csv'  \n",
    "procesar_csv(ruta_archivo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar Filas con Polaridad Neutra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo filtrado creado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lee el archivo CSV\n",
    "df = pd.read_csv(\"../Segment_extraction/nuevo_archivo.csv\", sep=\";\") \n",
    "\n",
    "# Filtra las filas donde la columna 'polaridad' no es igual a 2\n",
    "df_filtrado = df[df[\"polarity\"] != 'neutral']\n",
    "\n",
    "# Guarda el DataFrame filtrado en un nuevo archivo CSV\n",
    "df_filtrado.to_csv(\"archivo_filtrado.csv\", sep=\";\", index=False)\n",
    "\n",
    "print(\"Archivo filtrado creado correctamente.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
